# ---- 1-2 Libraries for Configuration and Modules ----
from codes.config.config_cnn import TrainConfig
from codes.function.Dataset import ImageDataset
import codes.function.Loss as lossfunction
from codes.function.Log import log
from codes.models.DIFFUSION import positional_encoding
from codes.models.DIFFUSION import prepare_data
# ---- 1-3 Libraries for pytorch and others ----
import torch
import torch.nn as nn
import torch.cuda
from torch.utils.data import DataLoader,random_split, ConcatDataset
import torch.nn.functional as F
import importlib
import matplotlib.pyplot as plt
import numpy as np
import time
from datetime import timedelta
import os
from typing import List, Optional
from torch.nn.utils import clip_grad_norm_

def format_model_params(params: dict, indent: int = 8) -> str:
    max_len = max(len(str(k)) for k in params)
    pad = " " * indent  # æ§åˆ¶ç¼©è¿›å®½åº¦
    return "\n".join(
        f"{pad}â€¢ {k:<{max_len}} : {v}"
        for k, v in params.items()
    )

def train_cnn(
    model,
    optimizer,
    scheduler,
    criterion,
    device,
    trainloader,
    testloader,
    num_epochs,
    logger,
    logpath,
    train_msg="",
    LOSS_PLOT=[],
    TESTLOSS_PLOT=[],
    EPOCH_PLOT=[],
    Best_model_save_path=""
):
    """
    å‡½æ•°ç‰¹è‰²ï¼šä½¿ç”¨scheduleråŠ¨æ€è°ƒæ•´å­¦ä¹ ç‡ã€‚ä¼¼ä¹æ²¡æœ‰å…¶ä»–åœ¨trainå‡½æ•°è¿›è¡Œä¼˜åŒ–çš„æ–¹å¼ï¼Ÿ
    """
    with open(logpath, "w", encoding="utf-8"):
        pass
    log(logpath,train_msg)

    best_loss = float('inf')

    for epoch in range(num_epochs):
        model.train()
        total_loss = 0.0
        current_lr = scheduler.get_last_lr()[0]

        for _, (img_LR, img_HR) in enumerate(trainloader):
            img_LR = img_LR.to(device)
            img_HR = img_HR.to(device)
            img_SR, _, _ = model(img_LR)
            loss = criterion(img_SR, img_HR)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            total_loss += loss.item()

        avg_loss = total_loss / len(trainloader)

        # ===== éªŒè¯é˜¶æ®µ =====
        test_loss = 0.0
        model.eval()
        with torch.no_grad():
            for _, (img_LR, img_HR) in enumerate(testloader):
                img_LR = img_LR.to(device)
                img_HR = img_HR.to(device)
                img_SR, _, _ = model(img_LR)
                loss = criterion(img_SR, img_HR)
                test_loss += loss.item()
        test_avg_loss = test_loss / len(testloader)

        # ===== æ—¥å¿—è¾“å‡º =====
        logger.info(f"Epoch [{epoch+1}/{num_epochs}], Avg Loss: {avg_loss:.4e}, Test Loss: {test_avg_loss:.4e}, LR: {current_lr:.4e}")
        log(logpath,f"Epoch [{epoch+1}/{num_epochs}], Avg Loss: {avg_loss:.4e}, Test Loss: {test_avg_loss:.4e}, LR: {current_lr:.4e}")

        LOSS_PLOT.append(avg_loss)
        TESTLOSS_PLOT.append(test_avg_loss)
        EPOCH_PLOT.append(epoch)

        # ===== ä¿å­˜æœ€ä½³æ¨¡å‹ =====
        if test_avg_loss < best_loss:
            best_loss = test_avg_loss
            torch.save(model.state_dict(), Best_model_save_path)
            logger.success(f"ğŸ’¾ å‘ç°æ›´ä¼˜æ¨¡å‹ï¼ˆTest Loss={best_loss:.4e}ï¼‰ï¼Œå·²ä¿å­˜ï¼š{Best_model_save_path} (Epoch {epoch+1})")

        scheduler.step()

def train_diffusion(
    unet,                       # å™ªå£°é¢„æµ‹ç½‘ç»œï¼ˆå¦‚ AttentionUNet åŒ…è£…ï¼‰
    diffusion,                  # ä½ çš„ DDPM_Transformer / DDPM å®ä¾‹
    optimizer,
    scheduler,
    criterion,                  # ä¸€èˆ¬ç”¨ nn.MSELoss()ï¼Œæ¯”è¾ƒ predicted_noise vs true_noise
    device,
    trainloader,
    testloader,
    num_epochs: int,
    logger,
    logpath: str,
    train_msg: str = "",
    LOSS_PLOT: Optional[List[float]] = None,
    TESTLOSS_PLOT: Optional[List[float]] = None,
    EPOCH_PLOT: Optional[List[int]] = None,
    Best_model_save_path: str = "",
    grad_clip: Optional[float] = None,  # e.g., 1.0ï¼›None è¡¨ç¤ºä¸è£å‰ª
    eval_sample_every: Optional[int] = None,  # e.g., 10ï¼›None è¡¨ç¤ºä¸åšé‡‡æ ·è¯„ä¼°
    save_time_steps: Optional[List[int]] = None,  # éœ€è¦å¯è§†åŒ–ä¸­é—´è¿‡ç¨‹æ—¶ä¼ å…¥
):
    """
    è®­ç»ƒ DDPMï¼ˆå™ªå£°é¢„æµ‹ï¼‰ç‰ˆçš„è¶…åˆ†è¾¨ï¼š
    - è®­ç»ƒç›®æ ‡ï¼šmin E[ || eps_pred - eps_true ||^2 ]
    - éªŒè¯åŒè®­ç»ƒï¼ˆä¸èµ°å®Œæ•´åå‘é‡‡æ ·ï¼Œé€Ÿåº¦å¿«ä¸”ç¨³å®šï¼‰
    - å¯é€‰ï¼šæ¯ N ä¸ª epoch è¿›è¡Œä¸€æ¬¡å°æ ·æœ¬åæ¨é‡‡æ ·ï¼ˆè€—æ—¶ï¼Œé»˜è®¤å…³é—­ï¼‰

    è¯´æ˜ï¼š
    - diffusion.sample_training_batch ä¼šå®Œæˆï¼š
        1) é‡‡æ · t
        2) å‰å‘åŠ å™ªå¾—åˆ° x_t ä¸çœŸå®å™ªå£° noise
        3) è‹¥ conditional=Trueï¼Œåˆ™å°† input_img ä¸ x_t åœ¨é€šé“ç»´æ‹¼æ¥
        4) è¿”å› (x_t_concat, t_emb, noise)
    """

    # --- åˆå§‹åŒ–ç»˜å›¾ç¼“å­˜ ---
    LOSS_PLOT = [] if LOSS_PLOT is None else LOSS_PLOT
    TESTLOSS_PLOT = [] if TESTLOSS_PLOT is None else TESTLOSS_PLOT
    EPOCH_PLOT = [] if EPOCH_PLOT is None else EPOCH_PLOT

    # --- é‡ç½®æ—¥å¿—æ–‡ä»¶ ---
    with open(logpath, "w", encoding="utf-8"):
        pass
    log(logpath, train_msg)

    best_loss = float('inf')

    for epoch in range(num_epochs):
        unet.train()
        total_loss = 0.0
        current_lr = scheduler.get_last_lr()[0]

        # ====== è®­ç»ƒ ======
        for _, (img_LR, img_HR) in enumerate(trainloader):
            img_LR = img_LR.to(device)
            img_HR = img_HR.to(device)

            # å‡†å¤‡å¸¦å™ªè¾“å…¥ä¸æ—¶é—´æ­¥åµŒå…¥
            x_t, t_emb, noise_true = diffusion.sample_training_batch(
                input_img=img_LR, target_img=img_HR
            )

            # é¢„æµ‹å™ªå£°
            noise_pred = unet(x_t, t_emb)

            loss = criterion(noise_pred, noise_true)

            optimizer.zero_grad(set_to_none=True)
            loss.backward()

            if grad_clip is not None and grad_clip > 0:
                clip_grad_norm_(unet.parameters(), max_norm=grad_clip)

            optimizer.step()
            total_loss += loss.item()

        avg_loss = total_loss / max(1, len(trainloader))

        # ====== éªŒè¯ ======
        test_loss = 0.0
        unet.eval()
        with torch.no_grad():
            for _, (img_LR, img_HR) in enumerate(testloader):
                img_LR = img_LR.to(device)
                img_HR = img_HR.to(device)

                x_t, t_emb, noise_true = diffusion.sample_training_batch(
                    input_img=img_LR, target_img=img_HR
                )
                noise_pred = unet(x_t, t_emb)
                loss = criterion(noise_pred, noise_true)
                test_loss += loss.item()

        test_avg_loss = test_loss / max(1, len(testloader))

        # ====== æ—¥å¿— ======
        logger.info(
            f"[DDPM] Epoch [{epoch+1}/{num_epochs}] "
            f"Train: {avg_loss:.4e} | Val: {test_avg_loss:.4e} | LR: {current_lr:.4e}"
        )
        log(
            logpath,
            f"[DDPM] Epoch [{epoch+1}/{num_epochs}] "
            f"Train: {avg_loss:.4e} | Val: {test_avg_loss:.4e} | LR: {current_lr:.4e}"
        )

        LOSS_PLOT.append(avg_loss)
        TESTLOSS_PLOT.append(test_avg_loss)
        EPOCH_PLOT.append(epoch)

        # ====== ä¿å­˜æœ€ä¼˜æƒé‡ï¼ˆæŒ‰éªŒè¯é›†å™ªå£°é¢„æµ‹æŸå¤±ï¼‰ ======
        if test_avg_loss < best_loss and Best_model_save_path:
            best_loss = test_avg_loss
            torch.save(unet.state_dict(), Best_model_save_path)
            logger.success(
                f"ğŸ’¾ å‘ç°æ›´ä¼˜æ¨¡å‹ï¼ˆVal NoiseLoss={best_loss:.4e}ï¼‰ï¼Œå·²ä¿å­˜ï¼š{Best_model_save_path} (Epoch {epoch+1})"
            )

        # ====== å¯é€‰ï¼šå°æ ·æœ¬åå‘é‡‡æ ·ï¼ˆæ…¢ï¼Œé»˜è®¤å…³é—­ï¼‰ ======
        do_sample = (eval_sample_every is not None) and ((epoch + 1) % eval_sample_every == 0)
        if do_sample:
            try:
                # å–ä¸€ä¸ªå° batch åšç¤ºä¾‹ï¼ˆæ¯”å¦‚å‰ 4 å¼ ï¼‰
                img_LR, img_HR = next(iter(testloader))
                img_LR = img_LR.to(device)
                img_HR = img_HR.to(device)
                n = min(4, img_LR.size(0))
                # åå‘æ‰©æ•£ï¼ˆæ¡ä»¶é‡‡æ ·ï¼‰
                samples = diffusion.reverse_diffusion(
                    model=unet,
                    n_images=n,
                    n_channels=img_HR.size(1),
                    input_image=img_LR[:n] if getattr(diffusion, "conditional", True) else None,
                    save_time_steps=save_time_steps
                )
                # è¿™é‡Œä¸å¼ºåˆ¶è®¡ç®— PSNR/SSIMï¼ˆå› æ•°æ®å½’ä¸€åŒ–ä¸åŒï¼‰ï¼Œå¦‚éœ€å¯åœ¨æ­¤å¤„æ·»åŠ 
                logger.info(f"[DDPM] Epoch {epoch+1}: é‡‡æ ·å®Œæˆï¼ˆç¤ºä¾‹ n={n}ï¼‰")
            except Exception as e:
                logger.warning(f"[DDPM] é‡‡æ ·å¤±è´¥ï¼ˆè·³è¿‡ï¼‰ï¼š{e}")

        scheduler.step()