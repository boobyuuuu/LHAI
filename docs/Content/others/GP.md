# 组会与进展

## 讨论 9月9日

要解决的几个数据问题

1. xingwei数据问题

2. 圆环/圆盘问题

3. 数据集引用的顺序问题

4. 分子云数据

5. HESS真实评估数据集

讨论结果：

1. xingwei数据问题：

    - 出现噪点数据。为什么会出现这种数据：输入的数据总流强太低，达到了仪器的噪声水平。解决：1. 考察一下产生这种噪点的输入数据流强，是否与其他数据有数量级的本质差别，并且确定产生这种噪点的输入数据的流强范围。  -- 第一个置信度控制量：输入总流强
    - 卷积出现流强分布异常问题。现象：在原本没有信号的地方出现了异常的流强峰值。我们猜测的原因：输入数据存在流强数量级差别过大的源，一些不明显的源导致。解决：考察一下产生这种异常的输入数据流强，是否与其他数据有数量级的本质差别，并且确定产生这种异常的输入数据的流强数量级差范围，确保现实应用中不会出现这个问题  -- 第二个置信度控制量：输入数据中源的流强数量级差

2. 圆环/圆盘问题

    - 确定会出现问题。猜测影响因素：1.内外径比例 2.外径绝对大小 3. 总流强 解决：1.内外径比例来控制圆环，生成连续圆环评估数据集，确定在多少时会出现问题。2.生成单个连续圆环评估数据集。 -- 第三个置信度考察量：圆环/圆盘评估值
  
3. 数据集引用的顺序问题。已解决，将shuffle设置为False

4. 分子云数据。已解决，分子云数据已经加入训练集

5. HESS真实评估数据集。不着急

## 组会七 9月7日

## 组会六 7月25日

1. 本周结营测试，没做多少任务
2. 完成了diffusion的框架，整合了一下现在能够用来应用展示的模型

CNN+DIFFUSION

xingwei+halos+miragesearch

一共6个模型，等待欣雨那边给数据进行实际检测

3. 后续我希望加深一点对模型的理解，因此会按照SR模型发展的顺序，对每一个模型在我们的框架的基础上进行实现，并且记录每个模型的优劣势和效果。相应的笔记会发布在网页上供大家参考。但是会以老师和欣雨那边的需要实现作为主要的工作内容。



## 组会五 7月13日

主要内容：讨论当前进展以及接下来的工作

---

欣雨：可以增加更多源的模式

工作：

一 结合实际

1. 将现在的模型应用到实际的天体源中

2. 与传统处理方法进行对比

二 迭代模型

十几个的数据：进行测试

4200个的数据：

## 组会四 6月24日

主要内容：星维、陈力琰、胡欣雨、刘梓航阶段性总结之前工作进展，并进行未来展望

---

可以从以下几个方面进行优化：

1. Model

    （1）尝试transformer

    （2）尝试resnet（残差网络）

2. LOSS

    （1）应对高/低流强差的情况：加入log loss

    （2）每个格点都不要差过10%：加框方差

    （3）加入物理机制/约束：LOSS加物理机制限制（非常FURTHER）

3. Data

    （1）整理目前的训练-验证Dataset，目前数据有点太乱，建立数据库

    （2）数据集清洗工作。建立数据预处理流程框架

4. Evaluation

    （1）加个框再进行评估，验证LOSS-（2）的效果

    （2）根据人工打分标签，训练出 evaluation model（非常FURTHER）

5. 训练机制：半监督方法

    （1）自我循环迭代半监督

    （2）根据人工打分标签，训练出 evaluation model；根据eval model进行大量数据集生成；进行循环训练（input data足够多）

6. Structure

    现在模型有点杂多，想个办法把不同的算法搞到同一个框架里去（非常重要且急迫）

## 组会三

可以从以下几个方面进行优化：

1. Model

    （1）尝试transformer

    （2）尝试resnet（残差网络）

2. LOSS

    （1）应对高/低流强差的情况：加入log loss

    （2）每个格点都不要差过10%：加框方差

    （3）加入物理机制/约束：LOSS加物理机制限制（非常FURTHER）

3. Data


4. Evaluation

    （1）加个框再进行评估，验证LOSS-（2）的效果

    （2）根据人工打分标签，训练出 evaluation model（非常FURTHER）

5. 训练机制：半监督方法

    （1）自我循环迭代半监督

    （2）根据人工打分标签，训练出 evaluation model；根据eval model进行大量数据集生成；进行循环训练（input data足够多）

